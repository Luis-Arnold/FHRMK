---
title: "RMKAgeing"
format: html
editor: visual
---

Methodology

Describe Data

# Preconditions

### Install Packages

```{r}

if (!requireNamespace("readr", quietly = TRUE)) {
  install.packages("readr")
}
if (!requireNamespace("dplyr", quietly = TRUE)) {
  install.packages("dplyr")
}
if (!requireNamespace("caret", quietly = TRUE)) {
  install.packages("caret")
}
if (!requireNamespace("leaps", quietly = TRUE)) {
  install.packages("leaps")
}
if (!requireNamespace("corrplot", quietly = TRUE)) {
  install.packages("corrplot")
}
if (!requireNamespace("tidyr", quietly = TRUE)) {
  install.packages("tidyr")
}
if (!requireNamespace("randomForest", quietly = TRUE)) {
  install.packages("randomForest")
}
if (!requireNamespace("ggplot2", quietly = TRUE)) {
  install.packages("ggplot2")
}
if (!requireNamespace("car", quietly = TRUE)) {
  install.packages("car")
}
if (!requireNamespace("plotly", quietly = TRUE)) {
  install.packages("plotly")
}
if (!requireNamespace("parallel", quietly = TRUE)) {
  install.packages("parallel")
}
if (!requireNamespace("xgboost", quietly = TRUE)) {
  install.packages("xgboost")
}
if (!requireNamespace("readxl", quietly = TRUE)) {
  install.packages("readxl")
}
if (!requireNamespace("countrycode", quietly = TRUE)) {
  install.packages("countrycode")
}
```

### Load Packages

```{r}

library(plotly)
library(corrplot)
library(readr)
library(caret)
library(leaps)
library(tidyr)
library(dplyr)
library(randomForest)
library(ggplot2)
library(car)
library(parallel)
library(xgboost)
library(readxl)
library(countrycode)
```

### Load Dataset

Share dataset and [World Development Indicators](https://datacatalog.worldbank.org/search/dataset/0037712/World-Development-Indicators?) of the World Bank Group

```{r}

SHARE_path <- "C:/Users/luisj/Documents/Github/FHRMK/RawData/easySHARE_rel9-0-0_R/easySHARE_rel9_0_0.rda"

WDIfinancial_path <- "C:/Users/luisj/Documents/Github/FHRMK/RawData/WDI_CSV_2024_10_24/WDICSV.csv"

WDIcountry_path <- "C:/Users/luisj/Documents/Github/FHRMK/RawData/WDI_CSV_2024_10_24/WDICountry.csv"

global_crisis_data_path <- "C:/Users/luisj/Documents/Github/FHRMK/RawData/20160923_global_crisis_data.xlsx"

#setwd(filePath)

load(SHARE_path)

WDIfinancial_dataset <- read.csv(WDIfinancial_path, header = TRUE, sep = ",")

WDIcountry_dataset <- read.csv(WDIcountry_path, header = TRUE, sep = ",")

global_crisis_data <- read_excel(global_crisis_data_path, na = c("",
                                                                 "n/a",
                                                                 "N/A",
                                                                 "n.a.",
                                                                 "NA"))

rm(SHARE_path, WDIfinancial_path, global_crisis_data_path)
```

# Clean Data

Get European country tags of financial Data

```{r}

if (exists("WDIcountry_dataset")) {
  CountryTags <- WDIcountry_dataset %>%
    filter(Other.groups == "Euro area"
           | Country.Code == "CHE"
           | Country.Code == "ISR") %>%
    select(Country.Code)
  
  rm(WDIcountry_dataset) #CLEANUP
}

head(CountryTags)
```

Select relevant Columns and rows, remove irrelevant Countries from Crisis Data

Remove data older than 20 years before the birth of the oldest participant.

```{r}

relevant_first_year <- min(easySHARE_rel9_0_0$int_year - easySHARE_rel9_0_0$age, na.rm = TRUE) - 20

if (exists("global_crisis_data")) {
  EU_crisis_dataset <- global_crisis_data %>%
    select("CC3",
           "Country",
           "Year",
           "Banking Crisis",
           "Systemic Crisis",
           "Gold Standard",
           "Currency Crises",
           "Inflation Crises") %>%
    filter(Year >= relevant_first_year) %>%
    inner_join(CountryTags, global_crisis_data, by = c("CC3" = "Country.Code"))
  
  rm(global_crisis_data) #CLEANUP
}

EU_crisis_dataset %>%
  distinct(Country, .keep_all = TRUE) %>%
  head(6)
```

Select relevant financial indicators

-   GDP per capita, PPP (current international \$) -\> Comparing living standards and economic well being. (NY.GDP.PCAP.PP.CD)

-   GDP, PPP (current international \$) -\> Comparing size of economies, adjusted for price differences. (NY.GDP.MKTP.PP.CD)

-   GDP growth (annual %) -\> Simple, raw comparison of income and economic output. (NY.GDP.MKTP.KD.ZG)

Remove irrelevant Countries from WDI Data set

```{r}

if (exists("WDIfinancial_dataset")) {
  EUfinancial_dataset <- WDIfinancial_dataset %>%
    filter(Indicator.Code == "NY.GDP.PCAP.PP.CD" |
           Indicator.Code == "NY.GDP.MKTP.PP.CD" |
           Indicator.Code == "NY.GDP.MKTP.KD.ZG") %>%
    inner_join(CountryTags, by = "Country.Code")
  
  rm(WDIfinancial_dataset) #CLEANUP
}

head(EUfinancial_dataset)
```

### Tidy up numeric in SHARE

```{r}

NA_counts <- colSums(is.na(easySHARE_rel9_0_0))
NA_counts[NA_counts > 0]
```

### Impute Values with median

```{r}

easySHARE_rel9_0_0$income_pct_w9[is.na(easySHARE_rel9_0_0$income_pct_w9)] <- median(easySHARE_rel9_0_0$income_pct_w9, na.rm = TRUE)

NA_counts <- colSums(is.na(easySHARE_rel9_0_0))
NA_counts[NA_counts > 0]
```

Count number of participants with Big 5 personality trait answers

```{r}

head(easySHARE_rel9_0_0)
```

```{r}

citizens_with_bfp <- easySHARE_rel9_0_0 %>%
  filter(birth_country %in% citizenship &
         birth_country == country_mod) %>%
  filter(bfi10_extra_mod > 0 &
         bfi10_agree_mod > 0 &
         bfi10_consc_mod > 0 &
         bfi10_neuro_mod > 0 &
         bfi10_open_mod > 0) %>%
  distinct(mergeid, .keep_all = TRUE) %>%
  summarise(count = n())

citizens_with_bfp
```

We look at distinct people

### Identify Citizens in SHARE

-   Birth country -\> Country born in.

-   Citizenship -\> Current citizenships (can be multiple).

-   Country Mod -\> Country as ISO Code.

```{r}

if (exists("easySHARE_rel9_0_0")) {
  citizens <- easySHARE_rel9_0_0 %>%
    filter(birth_country %in% citizenship &
           birth_country == country_mod) %>%
    filter(bfi10_extra_mod > 0 &
           bfi10_agree_mod > 0 &
           bfi10_consc_mod > 0 &
           bfi10_neuro_mod > 0 &
           bfi10_open_mod > 0) %>%
    distinct(mergeid, .keep_all = TRUE) %>%
    select("mergeid",
           "int_year",
           "age",
           "country_mod",
           "female",
           "bfi10_extra_mod",
           "bfi10_agree_mod",
           "bfi10_consc_mod",
           "bfi10_neuro_mod",
           "bfi10_open_mod") %>%
    mutate(birth_year = int_year - age)
  
  citizens$country_mod <- countrycode(citizens$country_mod, origin = "iso3n", destination = "iso3c")
  
  #rm(easySHARE_rel9_0_0)
}

head(citizens)
```

## Add columns - influenced by recessions at times in their lives

Add life ranges to share dataset

```{r}

horizontal_crises <- EU_crisis_dataset %>%
  pivot_longer(cols = c(`Banking Crisis`, `Currency Crises`, `Inflation Crises`), 
               names_to = "Crisis_Type", 
               values_to = "Crisis_Flag") %>%
  filter(Crisis_Flag == 1) %>%  # Keep only rows where the crisis occurred
  group_by(CC3, Crisis_Type) %>%
  summarise(Years = list(Year), .groups = "drop") %>%
  pivot_wider(names_from = Crisis_Type, values_from = Years)


citizens <- citizens %>%
  mutate(
    range_birth = Map(seq, birth_year - 7, birth_year + 5),
    range_childhood = Map(seq, birth_year + 6, birth_year + 10),
    range_teenage = Map(seq, birth_year + 13 - 2, birth_year + 13 + 2),
    range_early_adulthood = Map(seq, birth_year + 20 - 3, birth_year + 20 + 3),
    range_adulthood = Map(seq, birth_year + 30 - 5, birth_year + 30 + 5),
    range_retirement = Map(seq, birth_year + 50 - 10, birth_year + 30 + 10)
  )

matches <- citizens %>%
  left_join(horizontal_crises, by = c("country_mod" = "CC3")) %>%
  rowwise() %>%
  mutate(
    match_birth = as.integer(length(intersect(unlist(range_birth),
                                              unlist(`Banking Crisis`))) > 0),
    match_teenage = as.integer(length(intersect(unlist(range_teenage),
                                                unlist(`Banking Crisis`))) > 0),
    match_early_adulthood = as.integer(length(intersect(unlist(range_early_adulthood),
                                                        unlist(`Banking Crisis`))) > 0),
    match_adulthood = as.integer(length(intersect(unlist(range_adulthood),
                                                  unlist(`Banking Crisis`))) > 0)
  ) %>%
  ungroup() %>%
  select(mergeid,
         int_year,
         age,
         country_mod,
         female,
         bfi10_extra_mod,
         bfi10_agree_mod,
         bfi10_consc_mod,
         bfi10_neuro_mod,
         bfi10_open_mod,
         match_birth,
         match_teenage,
         match_early_adulthood,
         match_adulthood)

match_for_matrix <- matches %>%
  select(int_year,
         age,
         female,
         bfi10_extra_mod,
         bfi10_agree_mod,
         bfi10_consc_mod,
         bfi10_neuro_mod,
         bfi10_open_mod,
         match_birth,
         match_teenage,
         match_early_adulthood,
         match_adulthood)
```

Now these can be analyzed for the impact of recessions during times in their lives.

# Plotting

```{r}
correlation_matrix <- cor(match_for_matrix, use = "pairwise.complete.obs")

View(correlation_matrix)

png("correlation_plot_2.png", width = 1800, height = 1800)
```

```{r}

corrplot(correlation_matrix, method = "color", tl.col = "black", tl.srt = 45)
rm(correlation_matrix)
```

### Outliers \<look at\>

```{r}

# Let us check the outlier for each numerical column in the dataframe
## Convert df_3 to long format for easier faceting
df_long <- df_3 %>%
  select(loan_amnt, annual_inc, dti, delinq_2yrs, mths_since_last_record,
         inq_last_6mths, revol_util, tot_cur_bal, total_rev_hi_lim) %>%
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value")
## Create combined boxplot with faceting
ggplot(df_long, aes(x = variable, y = value)) +
  geom_boxplot(outlier.colour = "red", outlier.shape = 16, outlier.size = 2) +
  labs(title = "Boxplots of Numeric Columns", x = "Variable", y = "Value") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  facet_wrap(~ variable, scales = "free_y", ncol = 3)

# Based on the picture, it looks like only "loan_amnt" that not having outliers
# lets take out the outliers

# Some data we need to capping the outlier and other we can remove the outlier
# Based on the visualizer The one that will be remove is revol_util, total_rev_hi_lim, and dti
# Based on the visualizer The one that will be capping is annual_inc, delinq_2yrs and inq_last_6mths

# Helper function to calculate IQR bounds
get_iqr_bounds <- function(column) {
  Q1 <- quantile(column, 0.25, na.rm = TRUE)
  Q3 <- quantile(column, 0.75, na.rm = TRUE)
  IQR_value <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR_value
  upper_bound <- Q3 + 1.5 * IQR_value
  return(c(lower_bound, upper_bound))
}
# Step 1: Create `df_3_no_outliers` by removing outliers for specified columns
df_3_no_outliers <- df_3
for (col in c("revol_util", "total_rev_hi_lim", "dti")) {
  bounds <- get_iqr_bounds(df_3_no_outliers[[col]])
  df_3_no_outliers <- df_3_no_outliers[df_3_no_outliers[[col]] >= bounds[1] & 
                                         df_3_no_outliers[[col]] <= bounds[2], ]
}
# Step 2: Create `df_3_capped` by capping outliers for specified columns
df_3_capped <- df_3_no_outliers  # Start from `df_3_no_outliers` for consistency
for (col in c("annual_inc", "delinq_2yrs", "inq_last_6mths")) {
  bounds <- get_iqr_bounds(df_3_capped[[col]])
  df_3_capped[[col]] <- ifelse(df_3_capped[[col]] < bounds[1], bounds[1],
                               ifelse(df_3_capped[[col]] > bounds[2], bounds[2], df_3_capped[[col]]))
}
# Step 3: Assign the final result to `df_4` and remove unnevessary dataframe
df_4 <- df_3_capped
rm(df_3_capped,df_3_no_outliers)

# lets check the visulalizer again
## Convert df_4 to long format for easier faceting
df_long <- df_4 %>%
  select(loan_amnt, annual_inc, dti, delinq_2yrs, mths_since_last_record,
         inq_last_6mths, revol_util, tot_cur_bal, total_rev_hi_lim) %>%
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value")
## Create combined boxplot with faceting
ggplot(df_long, aes(x = variable, y = value)) +
  geom_boxplot(outlier.colour = "red", outlier.shape = 16, outlier.size = 2) +
  labs(title = "Boxplots of Numeric Columns", x = "Variable", y = "Value") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  facet_wrap(~ variable, scales = "free_y", ncol = 3)

# Based on the second picture, it will be best, if we just not use tot_cur_bal and mths_since_last_record
df_5 <- df_4 %>% select(-tot_cur_bal, -mths_since_last_record)
```

# Train Model

### Cross Validation (K-METHOD)

```{r}

set.seed(1)

match_df <- subset(match_for_matrix)

train_index <- sample(1:nrow(match_df), 0.9 * nrow(match_df))
train_data <- match_df[train_index, ]
validation_data <- match_df[-train_index, ]

control <- trainControl(method = "cv", number = 10)

df <- subset(match_df, select = -bfi10_extra_mod)

train_index <- sample(1:nrow(df), 0.9 * nrow(df))
train_data <- df[train_index, ]
validation_data <- df[-train_index, ]

model <- train(match_birth ~ ., data = train_data, method = "lm", trControl = control)

predictions <- predict(model, newdata = validation_data)

mse <- mean((validation_data$int_rate - predictions)^2)
mse
```

### Checking Multicolinearity once more

```{r}

# There is this warning message:
# Warning message:
#   In predict.lm(modelFit, newdata) :
#   prediction from rank-deficient fit; attr(*, "non-estim") has doubtful cases
# There is perhaps something to do with the multicoloniarity

# Checking multicollinearity
final_model <- model$finalModel
vif(final_model)

# In this case:
  
# Some variables like home_ownershipMORTGAGE,
# home_ownershipOWN, and home_ownershipRENT
# have very high VIF values (e.g., 1.7e+05, 1.65e+05).
# These very high values likely indicate that the variables are highly correlated
# with each other, which could lead to multicollinearity problems.

# The other predictors like loan_amnt, annual_inc, dti, inq_last_6mths,
# and revol_util have VIF values around 1, indicating no multicollinearity.

# Let see again the cor matrix
# USing plotly so you can zoom in and out easly for the plot

# Convert correlation matrix to a long format for plotly
cor_matrix <- cor(df_4)
cor_matrix_long <- as.data.frame(as.table(cor_matrix))

# Create an interactive heatmap
plot_ly(data = cor_matrix_long, x = ~Var1, y = ~Var2, z = ~Freq, 
        type = "heatmap", colors = colorRampPalette(c("blue", "white", "red"))(100)) %>%
  layout(title = "Correlation Matrix", xaxis = list(title = ""), 
         yaxis = list(title = ""))
# You can zoom in and out easly and you can make the things bigger or smaller

# Based on the picture, we can say that these are the variable having multicollinearity
# homeowenrship_RENT -- home_ownershipMORTGAGE
# purposedebt_consolidation -- purposecredit_card
# We can try to delete one of them
# in here these are the column that we delete homeowenrship_RENT & purposedebt_consolidation
# lets take the dataframe from df_5 and put in df_6 with also remove some things

df_6 <- df_5 %>%
  select(-`term60 months`, 
         -home_ownershipRENT, 
         -purposedebt_consolidation, 
         -delinq_2yrs)
```

### Cross Validation (K-METHOD) 2.0

```{r}

# Getting DF for Linear Regression Model
df <- df_6

# Sample data (replace with your actual df)
set.seed(1)  # Set seed for reproducibility

# Split df into 90% training and 10% validation
train_index <- sample(1:nrow(df), 0.9 * nrow(df))  # Get 90% of row indices for training
train_data <- df[train_index, ]  # Training data
validation_data <- df[-train_index, ]  # Validation data

# Set up cross-validation control (e.g., 10-fold)
control <- trainControl(method = "cv", number = 10)

# Split df into 90% training and 10% validation
train_index <- sample(1:nrow(df), 0.9 * nrow(df))  # Get 90% of row indices for training
train_data <- df[train_index, ]  # Training data
validation_data <- df[-train_index, ]  # Validation data

# Train the Linear Model using cross-validation
model <- train(int_rate ~ ., data = train_data, method = "lm", trControl = control)

# Calculate evaluation metrics on the validation set using predictions
predictions <- predict(model, newdata = validation_data)

# Calculate Evaluation Metrics
## Mean Squared Error (MSE)
mse <- mean((validation_data$int_rate - predictions)^2)
mse
# [1] 11.01383
```

### Random Forest (Regression)

```{r}

# Last things, Random forest,
# I kind of really exhausted

# Lets clean the column names
## Define a function to clean column names
clean_column_names <- function(data) {
  colnames(data) <- gsub(" ", "_", colnames(data))  # Replace spaces with underscores
  colnames(data) <- gsub("\\`", "", colnames(data))  # Remove backticks
  colnames(data) <- gsub("\\+", "_", colnames(data))  # Replace plus signs with underscores
  colnames(data) <- gsub("n/a", "_na", colnames(data))  # Replace 'n/a' with '_na'
  return(data)
}
## Apply the function to both datasets
train_data <- clean_column_names(train_data)
validation_data <- clean_column_names(validation_data)

# Random forest
model_rf <- randomForest(int_rate ~ ., data = train_data,
                         importance = FALSE,
                         ntree = 100,
                         mtry = 3, 
                         do.trace = TRUE, 
                         parallel = TRUE)

# Make predictions on the test data
predictions_rf <- predict(model_rf, newdata = validation_data)

# Calculate the MSE
mse_rf <- mean((predictions_rf - validation_data$int_rate)^2)
mse_rf
```

### Sample Stratified Method

```{r}

# My laptop is not good
# So, lets just take a sample
# Before taking the sample with stratisfied,
# We need to make our category references,
# I try to make it by divided the range automarically

# Divide income into 5 equal-width ranges
df_7 <- df_6 %>%
  mutate(income_range = cut(annual_inc, breaks = 5,
                            labels = c("Very Low", "Low", "Medium", "High", "Very High")))

# Lets make the sample data 25% from each income_range
df_sample <- df_7 %>%
  group_by(income_range) %>%
  sample_frac(size = 0.25) %>%
  ungroup()

# Do not forget to delete the income_range afterwards
# If dont, it will having a multicollinearity
df_sample <- subset(df_sample, select = -c(income_range))

# Lets try again, shall we?
```

### Random Forest

```{r}

# Split df into 90% training and 10% validation
train_index <- sample(1:nrow(df_sample), 0.9 * nrow(df_sample))  # Get 90% of row indices for training
train_data <- df_sample[train_index, ]  # Training data
validation_data <- df_sample[-train_index, ]  # Validation dat

# Lets clean the column names
## Define a function to clean column names
clean_column_names <- function(data) {
  colnames(data) <- gsub(" ", "_", colnames(data))  # Replace spaces with underscores
  colnames(data) <- gsub("\\`", "", colnames(data))  # Remove backticks
  colnames(data) <- gsub("\\+", "_", colnames(data))  # Replace plus signs with underscores
  colnames(data) <- gsub("n/a", "_na", colnames(data))  # Replace 'n/a' with '_na'
  return(data)
}
## Apply the function to both datasets
train_data <- clean_column_names(train_data)
validation_data <- clean_column_names(validation_data)

# Random forest
model_rf <- randomForest(int_rate ~ ., data = train_data,
                         importance = FALSE,
                         ntree = 500,
                         mtry = 3, 
                         do.trace = TRUE, 
                         parallel = TRUE)

# Make predictions on the test data
predictions_rf <- predict(model_rf, newdata = validation_data)

# Calculate the MSE
mse_rf <- mean((predictions_rf - validation_data$int_rate)^2)
mse_rf
```

### XGBoost

```{r}

# Lets Make the xgboost
dtrain <- xgb.DMatrix(data = as.matrix(train_data[,-1]), label = train_data$int_rate)
model_xgb <- xgboost(data = dtrain, max_depth = 10, eta = 0.1, nrounds = 100, objective = "reg:squarederror")

# Testing the xgboost data
## Prepare the test data (convert to xgboost DMatrix)
dtest <- xgb.DMatrix(data = as.matrix(df[,-1]))  # remove target column
## Generate predictions
predictions <- predict(model_xgb, dtest)
## Calculate MSE
actuals <- df$int_rate
mse <- mean((predictions - actuals)^2)
mse

# FINALLY -> 9 both training and validation
```
